# train <- df[-which(randomIndex == j),]
# test <-  df[ which(randomIndex == j),]
#
#
#
# n = nrow(ds)
# trainIndex = sample(1:n, size = round(0.8*n), replace=FALSE)
# train = ds[trainIndex ,]
# test = ds[-trainIndex ,]
#
# n1=nrow(train)
# vindex=sample(1:n1, size = round(0.75*n1), replace=FALSE)
# train1=train[vindex,]
# validation=train[-vindex,]
#
# nrow(validation)
# print(table(train[,ncol(train)]))
# print(table(test[,ncol(test)]))
#
slpit.res <- my.split(df=df,k=j)
train <- slpit.res[["df.train"]]
test <- slpit.res[["df.test"]]
valid <- slpit.res[["df.valid"]]
RFmodel <- randomForest(x = train[,retained.predictors], y = train[,"my_res"],
xtest = test[,retained.predictors], ytest = test[,"my_res"],
ntree = ntree,
mtry = mtry,
replace = replace,
nodesize = nodesize,
classwt = classwt,
cutoff = cutoff,
maxnodes = maxnodes)
confu <- as.data.frame(RFmodel$test$confusion)
TP   <-  TP + confu[labels[2], labels[2]]
FN   <-  FN + confu[labels[2], labels[1]]
FP   <-  FP + confu[labels[1], labels[2]]
TN   <-  TN + confu[labels[1], labels[1]]
TPR <- TP/(TP+FN)
TNR <- TN/(TN+FP)
BACC <- (TPR + TNR)/2
print(BACC)
results6 <- c(results6, BACC)
}
mean.bacc <- mean(results6)
print(mean.bacc)
# TPR1 <- TP/(TP+FN)
# TNR1 <- TN/(TN+FP)
# BACC1 <- (TPR + TNR)/2
# print(BACC1)
results5 <- c(results5, mean.bacc)
}
Sys.time()
design.resp <- add.response(design, results5)
design.resp
my.split <- function(df,k){
createIndex <- rep(1:10, length.out = nrow(df))
randomIndex <- sample(createIndex)
trainset <- df[-which(randomIndex == k),]
test <-  df[ which(randomIndex == k),]
n1=nrow(trainset)
vindex=sample(1:n1, size = round(0.75*n1), replace=FALSE)
train=trainset[vindex,]
validation=trainset[-vindex,]
print(str(train))
print(str(validation))
print(str(test))
return(list(
df.train = train,
df.valid = validation,
df.test = test
))
}
my.split <- function(df,k){
createIndex <- rep(1:10, length.out = nrow(df))
randomIndex <- sample(createIndex)
trainset <- df[-which(randomIndex == k),]
test <-  df[ which(randomIndex == k),]
n1=nrow(trainset)
vindex=sample(1:n1, size = round(0.75*n1), replace=FALSE)
train=trainset[vindex,]
validation=trainset[-vindex,]
print(str(train))
print(str(validation))
print(str(test))
return(list(
df.train = train,
df.valid = validation,
df.test = test
))
}
for (j in 1:10){
print(paste("fold",j,"of run", i))
slpi.res <- my.split(df=df,K=2)}
for (j in 1:10){
print(paste("fold",j))
slpi.res <- my.split(df=df,K=j)}
for (j in 1:10){
print(paste("fold",j))
slpi.res <- my.split(df=df,K=j)}
slpi.res <- my.split(df=df,K=2)
my.split <- function(df,k){
createIndex <- rep(1:10, length.out = nrow(df))
randomIndex <- sample(createIndex)
trainset <- df[-which(randomIndex == k),]
test <-  df[ which(randomIndex == k),]
n1=nrow(trainset)
vindex=sample(1:n1, size = round(0.75*n1), replace=FALSE)
train=trainset[vindex,]
validation=trainset[-vindex,]
print(str(train))
print(str(validation))
print(str(test))
return(list(
df.train = train,
df.valid = validation,
df.test = test
))
}
for (j in 1:10){
print(paste("fold",j))
slpi.res <- my.split(df=df,K=j)}
my.split <- function(df,K){
createIndex <- rep(1:10, length.out = nrow(df))
randomIndex <- sample(createIndex)
trainset <- df[-which(randomIndex == K),]
test <-  df[ which(randomIndex == K),]
n1=nrow(trainset)
vindex=sample(1:n1, size = round(0.75*n1), replace=FALSE)
train=trainset[vindex,]
validation=trainset[-vindex,]
print(str(train))
print(str(validation))
print(str(test))
return(list(
df.train = train,
df.valid = validation,
df.test = test
))
}
for (j in 1:10){
print(paste("fold",j))
slpi.res <- my.split(df=df,K=j)}
my.split <- function(df,K){
createIndex <- rep(1:10, length.out = nrow(df))
randomIndex <- sample(createIndex)
trainset <- df[-which(randomIndex == K),]
test <-  df[ which(randomIndex == K),]
seed(23548)
n1=nrow(trainset)
vindex=sample(1:n1, size = round(0.75*n1), replace=FALSE)
train=trainset[vindex,]
validation=trainset[-vindex,]
print("train")
print(str(train))
print("validation")
print(str(validation))
print("test")
print(str(test))
return(list(
df.train = train,
df.valid = validation,
df.test = test
))
}
for (j in 1:10){
print(paste("fold",j))
slpi.res <- my.split(df=df,K=j)}
my.split <- function(df,K){
createIndex <- rep(1:10, length.out = nrow(df))
randomIndex <- sample(createIndex)
trainset <- df[-which(randomIndex == K),]
test <-  df[ which(randomIndex == K),]
set.seed(23548)
n1=nrow(trainset)
vindex=sample(1:n1, size = round(0.75*n1), replace=FALSE)
train=trainset[vindex,]
validation=trainset[-vindex,]
print("train")
print(str(train))
print("validation")
print(str(validation))
print("test")
print(str(test))
return(list(
df.train = train,
df.valid = validation,
df.test = test
))
}
for (j in 1:10){
print(paste("fold",j))
slpi.res <- my.split(df=df,K=j)}
results5 <- c()
results6 <- c()
Sys.time()
for (i in 1:nrow(design)){
if (design$ntree[i] == -1)     {ntree <- 100} else {ntree <- 500}
if (design$mtry[i] == -1)      {mtry <- floor(log(ncol(df)-1))} else {mtry <- ceiling(sqrt(ncol(df)-1))}
if (design$replace[i] == -1)   {replace <- FALSE} else {replace <- TRUE}
if (design$nodesize[i] == -1)  {nodesize <- 1} else {nodesize <- floor(0.1*nrow(df))}
if (design$classwt[i] == -1)   {classwt <- c(1,10)} else {classwt <- c(10,1)}
if (design$cutoff[i] == -1)    {cutoff <- c(0.2, 1 -0.2)} else {cutoff <- c(0.8,1-0.8)}
if (design$maxnodes[i] == -1)  {maxnodes <- 5} else {maxnodes <- NULL}
#cross-validation
createIndex <- rep(1:10, length.out = nrow(df))
randomIndex <- sample(createIndex)
innerResults <- c()
TP   <-  0
FN   <-  0
FP   <-  0
TN   <-  0
for (j in 1:10){
print(paste("fold",j,"of run", i))
train <- df[-which(randomIndex == j),]
test <-  df[ which(randomIndex == j),]
# print(table(train[,ncol(train)]))
# print(table(test[,ncol(test)]))
#
RFmodel <- randomForest(x = train[,retained.predictors], y = train[,"my_res"],
xtest = test[,retained.predictors], ytest = test[,"my_res"],
ntree = ntree,
mtry = mtry,
replace = replace,
nodesize = nodesize,
classwt = classwt,
cutoff = cutoff,
maxnodes = maxnodes)
confu <- as.data.frame(RFmodel$test$confusion)
TP   <-  TP + confu[labels[2], labels[2]]
FN   <-  FN + confu[labels[2], labels[1]]
FP   <-  FP + confu[labels[1], labels[2]]
TN   <-  TN + confu[labels[1], labels[1]]
TPR <- TP/(TP+FN)
TNR <- TN/(TN+FP)
BACC <- (TPR + TNR)/2
print(BACC)
results6 <- c(results6, BACC)
}
mean.bacc <- mean(results6)
print(mean.bacc)
# TPR1 <- TP/(TP+FN)
# TNR1 <- TN/(TN+FP)
# BACC1 <- (TPR + TNR)/2
# print(BACC1)
results5 <- c(results5, mean.bacc)
}
Sys.time()
design.resp <- add.response(design, results5)
design.resp
createIndex = rep(1:10, length.out = nrow(df))
randomIndex = sample(createIndex)
trainset <- df[-which(randomIndex == 2),]
test <-  df[ which(randomIndex == 2),]
set.seed(23568)
n1=nrow(trainset)
vindex=sample(1:n1, size = round(0.75*n1), replace=FALSE)
train=trainset[vindex,]
validation=trainset[-vindex,]
RFmodel <- randomForest(x = train[,retained.predictors],
y = train[,"my_res"])
confu <- as.data.frame(RFmodel$test$confusion)
confu
predictions.train <- predict(
object  = RFmodel,
newdata = train[,retained.predictors]
);
predictions.validation <- predict(
object  = RFmodel,
newdata = validation[,retained.predictors]
);
predictions.test <- predict(
object  = RFmodel,
newdata = test[,retained.predictors]
);
cm = table(train[,"my_res"], predictions.train)
cm
cm1 = table(train[,"my_res"], predictions.train)
cm1
cm1
cm2 = table(validation[,"my_res"], predictions.validation)
cm2
cm3 = table(test[,"my_res"], predictions.test)
cm3
labels <- unique(df[,ncol(df)])
TP   <-  0
FN   <-  0
FP   <-  0
TN   <-  0
TP   <-  TP + cm3[labels[2], labels[2]]
FN   <-  FN + cm3[labels[2], labels[1]]
FP   <-  FP + cm3[labels[1], labels[2]]
TN   <-  TN + cm3[labels[1], labels[1]]
TP
FN
FP
TN
?table
x <- c(2,1)
y<- c(3,4)
x <- c(2,2,2,1,1,1,4,4,5,5,5,5)
y<- c(2,2,2,3,3,3,4,7,7,8,8,8)
table(x,y)
x <- c(2,2,2,1,1,1,1,1,2,2,2,2)
y<- c(2,2,2,1,1,1,1,2,1,2,1,2)
table(x,y)
cross.validation <- function(
DF.input = NULL,
k        = NULL) {
require(caret);
### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
#cross-validation
createInd = rep(1:10, length.out = nrow(DF.input))
randomInd = sample(createInd)
trainset = DF.input[-which(randomInd == k),]
train    = DF.input[-which(randomInd == k),]
test     = DF.input[ which(randomInd == k),]
validationInd = sample(1:nrow(trainset), size = round(0.75*nrow(trainset)), replace=FALSE)
train         = trainset[validationInd,]
validation    = trainset[-validationInd,]
#    cat("\n str(train)\n");
# print(str(train ));
#    cat("\n str(validation)\n");
# print( str(validation ));
# cat("\n str(test)\n");
# print(str(test ));
return( list(
DF.train = train,
DF.valid = validation,
DF.test  = test ))
}
results5 <- c()
results6 <- c()
Sys.time()
for (i in 1:nrow(design)){
if (design$ntree[i] == -1)     {ntree <- 100} else {ntree <- 500}
if (design$mtry[i] == -1)      {mtry <- floor(log(ncol(df)-1))} else {mtry <- ceiling(sqrt(ncol(df)-1))}
if (design$replace[i] == -1)   {replace <- FALSE} else {replace <- TRUE}
if (design$nodesize[i] == -1)  {nodesize <- 1} else {nodesize <- floor(0.1*nrow(df))}
if (design$classwt[i] == -1)   {classwt <- c(1,10)} else {classwt <- c(10,1)}
if (design$cutoff[i] == -1)    {cutoff <- c(0.2, 1 -0.2)} else {cutoff <- c(0.8,1-0.8)}
if (design$maxnodes[i] == -1)  {maxnodes <- 5} else {maxnodes <- NULL}
#cross-validation
createIndex <- rep(1:10, length.out = nrow(df))
randomIndex <- sample(createIndex)
innerResults <- c()
TP   <-  0
FN   <-  0
FP   <-  0
TN   <-  0
for (j in 1:10){
print(paste("fold",j,"of run", i))
# train <- df[-which(randomIndex == j),]
# test <-  df[ which(randomIndex == j),]
# print(table(train[,ncol(train)]))
# print(table(test[,ncol(test)]))
crossvalidate.res <- cross.validation(
DF.input = DF.input,
k        = j)
train      = crossvalidate.res[["DF.train"]];
validation = crossvalidate.res[["DF.valid"]];
test       = crossvalidate.res[["DF.test"]];
RFmodel <- randomForest(x = train[,retained.predictors], y = train[,"my_res"],
xtest = test[,retained.predictors], ytest = test[,"my_res"],
ntree = ntree,
mtry = mtry,
replace = replace,
nodesize = nodesize,
classwt = classwt,
cutoff = cutoff,
maxnodes = maxnodes)
confu <- as.data.frame(RFmodel$test$confusion)
TP   <-  TP + confu[labels[2], labels[2]]
FN   <-  FN + confu[labels[2], labels[1]]
FP   <-  FP + confu[labels[1], labels[2]]
TN   <-  TN + confu[labels[1], labels[1]]
TPR <- TP/(TP+FN)
TNR <- TN/(TN+FP)
BACC <- (TPR + TNR)/2
print(BACC)
results6 <- c(results6, BACC)
}
mean.bacc <- mean(results6)
print(mean.bacc)
# TPR1 <- TP/(TP+FN)
# TNR1 <- TN/(TN+FP)
# BACC1 <- (TPR + TNR)/2
# print(BACC1)
results5 <- c(results5, mean.bacc)
}
results5 <- c()
results6 <- c()
Sys.time()
for (i in 1:nrow(design)){
if (design$ntree[i] == -1)     {ntree <- 100} else {ntree <- 500}
if (design$mtry[i] == -1)      {mtry <- floor(log(ncol(df)-1))} else {mtry <- ceiling(sqrt(ncol(df)-1))}
if (design$replace[i] == -1)   {replace <- FALSE} else {replace <- TRUE}
if (design$nodesize[i] == -1)  {nodesize <- 1} else {nodesize <- floor(0.1*nrow(df))}
if (design$classwt[i] == -1)   {classwt <- c(1,10)} else {classwt <- c(10,1)}
if (design$cutoff[i] == -1)    {cutoff <- c(0.2, 1 -0.2)} else {cutoff <- c(0.8,1-0.8)}
if (design$maxnodes[i] == -1)  {maxnodes <- 5} else {maxnodes <- NULL}
#cross-validation
createIndex <- rep(1:10, length.out = nrow(df))
randomIndex <- sample(createIndex)
innerResults <- c()
TP   <-  0
FN   <-  0
FP   <-  0
TN   <-  0
for (j in 1:10){
print(paste("fold",j,"of run", i))
# train <- df[-which(randomIndex == j),]
# test <-  df[ which(randomIndex == j),]
# print(table(train[,ncol(train)]))
# print(table(test[,ncol(test)]))
crossvalidate.res <- cross.validation(
DF.input = df,
k        = j)
train      = crossvalidate.res[["DF.train"]];
validation = crossvalidate.res[["DF.valid"]];
test       = crossvalidate.res[["DF.test"]];
RFmodel <- randomForest(x = train[,retained.predictors], y = train[,"my_res"],
xtest = test[,retained.predictors], ytest = test[,"my_res"],
ntree = ntree,
mtry = mtry,
replace = replace,
nodesize = nodesize,
classwt = classwt,
cutoff = cutoff,
maxnodes = maxnodes)
confu <- as.data.frame(RFmodel$test$confusion)
TP   <-  TP + confu[labels[2], labels[2]]
FN   <-  FN + confu[labels[2], labels[1]]
FP   <-  FP + confu[labels[1], labels[2]]
TN   <-  TN + confu[labels[1], labels[1]]
TPR <- TP/(TP+FN)
TNR <- TN/(TN+FP)
BACC <- (TPR + TNR)/2
print(BACC)
results6 <- c(results6, BACC)
}
mean.bacc <- mean(results6)
print(mean.bacc)
# TPR1 <- TP/(TP+FN)
# TNR1 <- TN/(TN+FP)
# BACC1 <- (TPR + TNR)/2
# print(BACC1)
results5 <- c(results5, mean.bacc)
}
results5 <- c()
results6 <- c()
Sys.time()
for (i in 1:nrow(design)){
if (design$ntree[i] == -1)     {ntree <- 100} else {ntree <- 500}
if (design$mtry[i] == -1)      {mtry <- floor(log(ncol(df)-1))} else {mtry <- ceiling(sqrt(ncol(df)-1))}
if (design$replace[i] == -1)   {replace <- FALSE} else {replace <- TRUE}
if (design$nodesize[i] == -1)  {nodesize <- 1} else {nodesize <- floor(0.1*nrow(df))}
if (design$classwt[i] == -1)   {classwt <- c(1,10)} else {classwt <- c(10,1)}
if (design$cutoff[i] == -1)    {cutoff <- c(0.2, 1 -0.2)} else {cutoff <- c(0.8,1-0.8)}
if (design$maxnodes[i] == -1)  {maxnodes <- 5} else {maxnodes <- NULL}
#cross-validation
createIndex <- rep(1:10, length.out = nrow(df))
randomIndex <- sample(createIndex)
innerResults <- c()
TP   <-  0
FN   <-  0
FP   <-  0
TN   <-  0
for (j in 1:10){
print(paste("fold",j,"of run", i))
# train <- df[-which(randomIndex == j),]
# test <-  df[ which(randomIndex == j),]
# print(table(train[,ncol(train)]))
# print(table(test[,ncol(test)]))
crossvalidate.res <- cross.validation(
DF.input = df,
k        = j)
train      = crossvalidate.res[["DF.train"]];
validation = crossvalidate.res[["DF.valid"]];
test       = crossvalidate.res[["DF.test"]];
RFmodel <- randomForest(x = train[,retained.predictors], y = train[,"my_res"],
xtest = test[,retained.predictors], ytest = test[,"my_res"],
ntree = ntree,
mtry = mtry,
replace = replace,
nodesize = nodesize,
classwt = classwt,
cutoff = cutoff,
maxnodes = maxnodes)
confu <- as.data.frame(RFmodel$test$confusion)
TP   <-  TP + confu[labels[2], labels[2]]
FN   <-  FN + confu[labels[2], labels[1]]
FP   <-  FP + confu[labels[1], labels[2]]
TN   <-  TN + confu[labels[1], labels[1]]
TPR <- TP/(TP+FN)
TNR <- TN/(TN+FP)
BACC <- (TPR + TNR)/2
print(BACC)
results6 <- c(results6, BACC)
}
print(results6)
mean.bacc <- mean(results6)
print(mean.bacc)
# TPR1 <- TP/(TP+FN)
# TNR1 <- TN/(TN+FP)
# BACC1 <- (TPR + TNR)/2
# print(BACC1)
results5 <- c(results5, mean.bacc)
}
x <- c(2,2,2,1,1,1,1,1,2,2,2,2)
y<- c(2,2,2,1,1,1,1,2,1,2,1,2)
table(x,y)
(1.2)^6
